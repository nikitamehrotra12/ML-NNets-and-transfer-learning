{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport h5py\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\nimport os",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8498e44461ec6c6b4b2d23c117a34e19fd249b04",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "file=h5py.File('../input/MNIST_Subset.h5','r')\nimages=file['X'][:]\nlabels=file['Y'][:]\nimages_train, images_test, labels_train, labels_test = train_test_split(images, labels, test_size=0.3, random_state=42)\nprint('test-image-shape',images_test.shape,',train-image-shape',images_train.shape,'test-label-shape',labels_test.shape,',train-label-shape',labels_train.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f469aa030b48c2065bdad154cf28b450631bf978"
      },
      "cell_type": "code",
      "source": "label_train=np.where(labels_train==7,0,1)\nlabel_test=np.where(labels_test==7,0,1)\nlabel_train=label_train.reshape(1,9975)\nlabel_test=label_test.reshape(1,4276)\nprint(label_test.shape)\nprint(label_train.shape)\nimage_test=np.reshape(images_test,(784,4276))\nimage_train=np.reshape(images_train,(784,9975))\nprint(image_test.shape)\nprint(image_train.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# def initialize_params(layers_dims,m):\n#     parameters={}\n#     no_of_layers=len(layers_dims)\n#     for i in range(1,no_of_layers):\n#         parameters['W'+str(i)]=np.random.rand(layers_dims[i],layers_dims[i-1])/ np.sqrt(layers_dims[i-1])\n#         parameters['b'+str(i)]=np.zeros((layers_dims[i],m))         \n#     return parameters\n# def softmax(a):\n#     cache=a\n#     softm=np.exp(a)\n#     softm=softm/(np.sum(softm,keepdims=True))\n#     return softm,cache\n# def sigmoid(Z):\n#     cache=Z\n#     return 1.0/(1.0+np.exp(-Z)),cache\n# def relu(Z):\n#     cache=Z\n#     return np.max(0,Z),cache\n# def linear_forwrd(A_PREV,W,b):\n# #     print('A_PREV',A_PREV.shape)\n# #     print('W',W.shape)\n# #     print('b',b.shape)\n#     Z = np.add(np.matmul(W, A_PREV), b)\n#     cache=(A_PREV,W,b)\n# #     print('Z',Z.shape)\n#     return Z,cache\n# def linear_activation(A_PREV,W,b,activation):\n#     if activation==\"sigmoid\":\n#         Z,linear_cache=linear_forwrd(A_PREV,W,b)\n# #         print('Z',Z.shape)\n#         A,activation_cache=sigmoid(Z)\n# #         print('A',A.shape)\n#     elif activation==\"relu\":\n#         Z,linear_cache=linear_forwrd(A_PREV,W,b)\n#         A,activation_cache=relu(Z)\n#     elif activation==\"softmax\":\n#         Z,linear_cache=linear_forwrd(A_PREV,W,b)\n#         A,activation_cache=softmax(Z)\n#     cache=(linear_cache,activation_cache)\n#     return A,cache       \n# def forward_prop(images,parameters):\n#     caches=[]\n#     A=images\n#     L=len(parameters)//2\n#     for i in range(1,L+1):\n#         if(i==L):\n# #             print('called',i)\n#             A_PREV=A   \n# #             print('prev',(np.unique(A_PREV)))\n#             A,cache=linear_activation(A_PREV,parameters['W'+str(i)],parameters['b'+str(i)],'sigmoid')\n# #             print('new',np.unique(A))\n#             caches.append(cache)\n#         else:\n# #             print(i)\n#             A_PREV=A\n# #             print('prev',np.unique(A_PREV))\n#             A,cache=linear_activation(A_PREV,parameters['W'+str(i)],parameters['b'+str(i)],'sigmoid')\n# #             print('new',np.unique(A))\n#             caches.append(cache)\n#     return A,caches\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b14cac08ee6efa54c43960ed25fabb186d515529"
      },
      "cell_type": "code",
      "source": "def initialize_parameters_deep(layer_dims):\n    \"\"\"\n    Arguments:\n    layer_dims -- python array (list) containing the dimensions of each layer in our network\n    \n    Returns:\n    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n                    bl -- bias vector of shape (layer_dims[l], 1)\n    \"\"\"\n    \n    np.random.seed(1)\n    parameters = {}\n    L = len(layer_dims)            # number of layers in the network\n\n    for l in range(1, L):\n        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) / np.sqrt(layer_dims[l-1]) #*0.01\n        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n        \n        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n\n        \n    return parameters\n\ndef linear_forward(A_prev, W, b):\n    \"\"\"\n    Implement the linear part of a layer's forward propagation.\n\n    Arguments:\n    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n    b -- bias vector, numpy array of shape (size of the current layer, 1)\n\n    Returns:\n    Z -- the input of the activation function, also called pre-activation parameter \n    cache -- a python dictionary containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n    \"\"\"\n    \n    Z = np.add(np.matmul(W, A_prev), b)\n    \n    assert(Z.shape == (W.shape[0], A_prev.shape[1]))\n    cache = (A_prev, W, b)\n    \n    return Z, cache\n\ndef sigmoid(Z):\n    \"\"\"\n    Implements the sigmoid activation in numpy\n    \n    Arguments:\n    Z -- numpy array of any shape\n    \n    Returns:\n    A -- output of sigmoid(z), same shape as Z\n    cache -- returns Z as well, useful during backpropagation\n    \"\"\"\n    \n    A = 1/(1+np.exp(-Z))\n    cache = Z\n    \n    return A, cache\n\ndef softmax(a):\n    cache=a\n    softm=np.exp(a-np.max(a))\n    softm=softm/(np.sum(softm,keepdims=True))\n    return softm,cache\n\ndef relu(Z):\n    \"\"\"\n    Implement the RELU function.\n\n    Arguments:\n    Z -- Output of the linear layer, of any shape\n\n    Returns:\n    A -- Post-activation parameter, of the same shape as Z\n    cache -- a python dictionary containing \"A\" ; stored for computing the backward pass efficiently\n    \"\"\"\n    \n    A = np.maximum(0,Z)\n    \n    assert(A.shape == Z.shape)\n    \n    cache = Z \n    return A, cache\n\n## Linear-Activation Forward\ndef linear_activation_forward(A_prev, W, b, activation):\n    \"\"\"\n    Implement the forward propagation for the LINEAR->ACTIVATION layer\n\n    Arguments:\n    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n    b -- bias vector, numpy array of shape (size of the current layer, 1)\n    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n\n    Returns:\n    A -- the output of the activation function, also called the post-activation value \n    cache -- a python dictionary containing \"linear_cache\" and \"activation_cache\";\n             stored for computing the backward pass efficiently\n    \"\"\"\n    \n    if activation == \"sigmoid\":\n        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n        ### START CODE HERE ### (≈ 2 lines of code)\n        Z, linear_cache = linear_forward(A_prev, W, b)\n        A, activation_cache = sigmoid(Z)\n        ### END CODE HERE ###\n    \n    elif activation == \"softmax\":\n        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n        ### START CODE HERE ### (≈ 2 lines of code)\n        Z, linear_cache = linear_forward(A_prev, W, b)\n        A, activation_cache = softmax(Z)\n        ### END CODE HERE ###\n    \n    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n    cache = (linear_cache, activation_cache)\n\n    return A, cache\n\ndef L_model_forward(X, parameters):\n    \"\"\"\n    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n    \n    Arguments:\n    X -- data, numpy array of shape (input size, number of examples)\n    parameters -- output of initialize_parameters_deep()\n    \n    Returns:\n    AL -- last post-activation value\n    caches -- list of caches containing:\n                every cache of linear_activation_forward(A_prev, W, b, 'relu') (there are L-1 of them, indexed from 0 to L-2)\n                the cache of linear_activation_forward(A_prev, W, b, 'sigmoid') (there is one, indexed L-1)\n    \"\"\"\n\n    caches = []\n    A = X\n    L = len(parameters) // 2                  # number of layers in the neural network\n    \n    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n    for l in range(1, L):\n        A_prev = A \n        A, cache = linear_activation_forward(\n            A_prev, parameters['W' + str(l)], parameters['b' + str(l)], 'sigmoid'\n        )\n        caches.append(cache)\n    \n    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n    AL, cache = linear_activation_forward(\n            A, parameters['W' + str(L)], parameters['b' + str(L)], 'sigmoid'\n        )\n    caches.append(cache)\n\n    assert(AL.shape == (1,X.shape[1]))\n            \n    return AL, caches\n\n\ndef compute_cost(AL, Y):\n    \"\"\"\n    Implement the cost function defined by equation (7).\n\n    Arguments:\n    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n\n    Returns:\n    cost -- cross-entropy cost\n    \"\"\"\n    \n    m = Y.shape[1]\n   # print(AL.shape,Y.shape)\n    # Compute loss from aL and y.\n    cost = (1./m) * (-np.dot(Y,np.log(AL).T) - np.dot(1-Y, np.log(1-AL).T))\n    \n    cost = np.squeeze(cost)  \n#     print(cost.shape)\n    # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n    assert(cost.shape == ())\n    \n    return cost",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a981e4d3b109f31d48b9397eb64db18e695c4839",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "def linear_backward(dZ, cache):\n    A_prev, W, b = cache\n    m = A_prev.shape[1]\n    dW = (1.0/m) * np.matmul(dZ, A_prev.T)\n    db = (1.0/m) * np.sum(dZ, axis=-1, keepdims=True)\n    dA_prev = np.matmul(np.transpose(W), dZ) \n    assert (dA_prev.shape == A_prev.shape)\n    assert (dW.shape == W.shape)\n    assert (db.shape == b.shape)    \n    return dA_prev, dW, db\ndef softmax_backward(softmax,cache):\n    # Reshape the 1-d softmax to 2-d so that np.dot will do the matrix multiplication\n    s = softmax.reshape(-1,1)\n    return np.diagflat(s) - np.dot(s, s.T),cache\ndef sigmoid_backward(dA, cache):\n    Z = cache\n    s = 1/(1+np.exp(-Z))\n    dZ = dA * s * (1-s)\n    return dZ\ndef relu_backward(dA, cache):\n    Z = cache\n    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n    dZ[Z <= 0] = 0\n    assert (dZ.shape == Z.shape) \n    return dZ\ndef linear_activation_backward(dA, cache, activation):\n    linear_cache, activation_cache = cache   \n    if activation == \"relu\":\n        dZ = relu_backward(dA, activation_cache)\n        dA_prev, dW, db = linear_backward(dZ, linear_cache)        \n    elif activation == \"sigmoid\":\n        dZ = sigmoid_backward(dA, activation_cache)\n        dA_prev, dW, db = linear_backward(dZ, linear_cache)  \n    elif activation==\"softmax\":\n        dZ = softmax_backward(dA, activation_cache)\n        dA_prev, dW, db = linear_backward(dZ, linear_cache)  \n    return dA_prev, dW, db\ndef L_model_backward(AL, Y, caches):\n    grads = {}\n    L = len(caches) # the number of layers\n    m = AL.shape[1]\n#     print(m,AL.shape,L,Y.shape)\n    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL    \n    # Initializing the backpropagation\n    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))    \n    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"AL, Y, caches\". Outputs: \"grads[\"dAL\"], grads[\"dWL\"], grads[\"dbL\"]\n    current_cache = caches[L-1]\n    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, activation = \"sigmoid\")    \n    for l in reversed(range(L-1)):\n        # lth layer: (RELU -> LINEAR) gradients.\n        current_cache = caches[l]\n        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 2)], current_cache, activation = \"sigmoid\")\n        grads[\"dA\" + str(l + 1)] = dA_prev_temp\n        grads[\"dW\" + str(l + 1)] = dW_temp\n        grads[\"db\" + str(l + 1)] = db_temp\n    return grads\ndef update_parameters(parameters, grads, learning_rate):\n    L = len(parameters) // 2 # number of layers in the neural network\n    # Update rule for each parameter. Use a for loop.\n    for l in range(L):\n        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]        \n    return parameters\ndef L_layer_model(X, Y, layers_dims, parameters=None, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):#lr was 0.009\n    np.random.seed(1)\n    costs = []                         # keep track of cost\n    if not parameters:\n        parameters = initialize_parameters_deep(layers_dims)\n    for i in range(0, num_iterations):\n        AL, caches = L_model_forward(X, parameters)\n        # Compute cost.\n       # print('AL',AL.shape)\n        cost = compute_cost(AL, Y)\n        # Backward propagation.\n        grads = L_model_backward(AL, Y, caches)       \n        # Update parameters.\n        parameters = update_parameters(parameters, grads, learning_rate)                \n        # Print the cost every 100 training example\n        if print_cost and i % 10 == 0:\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n        if print_cost and i % 100 == 0:\n            costs.append(cost)            \n    # plot the cost\n    plt.plot(costs)\n    plt.ylabel('cost')\n    plt.xlabel('iterations (per tens)')\n    plt.title(\"Learning rate =\" + str(learning_rate))\n    plt.show()    \n    return parameters, costs\nlayers_dims=[784,100,1]\nparameters, costs = L_layer_model(image_train,label_train, layers_dims, num_iterations = 3000, print_cost = True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5ab123cde94b7fbe303be5746f1bd7aedc983953"
      },
      "cell_type": "code",
      "source": "def predict(X, y, parameters):\n    \"\"\"\n    This function is used to predict the results of a  L-layer neural network.\n    \n    Arguments:\n    X -- data set of examples you would like to label\n    parameters -- parameters of the trained model\n    \n    Returns:\n    p -- predictions for the given dataset X\n    \"\"\"\n    \n    m = X.shape[1]\n    n = len(parameters) // 2 # number of layers in the neural network\n    p = np.zeros((1,m))\n    \n    # Forward propagation\n    probas, caches = L_model_forward(X, parameters)\n\n    \n    # convert probas to 0/1 predictions\n    for i in range(0, probas.shape[1]):\n        if probas[0,i] > 0.5:\n            p[0,i] = 1\n        else:\n            p[0,i] = 0\n    \n    #print results\n    #print (\"predictions: \" + str(p))\n    #print (\"true labels: \" + str(y))\n    print(\"Accuracy: \"  + str(np.sum((p == y)/m)))\n        \n    return p\npred_train = predict(image_train,label_train, parameters)\npred_test = predict(image_test,label_test, parameters)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ab8ff098df9bed3d1b147d2fd8f5a7e26977057f"
      },
      "cell_type": "code",
      "source": "layers_dims=[784,100,50,50,1]\nparameters, costs = L_layer_model(image_train,label_train, layers_dims, num_iterations = 3000, print_cost = True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "71b4cf44a027fb237504148139aa76f229034bee"
      },
      "cell_type": "code",
      "source": "pred_train = predict(image_train,label_train, parameters)\npred_test = predict(image_test,label_test, parameters)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "93882224887ac92cce91d98c24a377d98480612c"
      },
      "cell_type": "code",
      "source": "# def print_mislabeled_images(classes, X, y, p):\n#     \"\"\"\n#     Plots images where predictions and truth were different.\n#     X -- dataset\n#     y -- true labels\n#     p -- predictions\n#     \"\"\"\n#     a = p + y\n#     mislabeled_indices = np.asarray(np.where(a == 1))\n#     plt.rcParams['figure.figsize'] = (40.0, 40.0) # set default size of plots\n#     num_images = len(mislabeled_indices[0])\n#     for i in range(num_images):\n#         index = mislabeled_indices[1][i]\n#         plt.subplot(2, num_images, i + 1)\n#         plt.imshow(X[:,index].reshape(28,28), interpolation='nearest')\n#         plt.axis('off')\n#         plt.title(\"Prediction: \" + classes[int(p[0,index])].decode(\"utf-8\") + \" \\n Class: \" + classes[y[0,index]].decode(\"utf-8\"))\n# classes = np.array([b'7',b'9'])\n# print_mislabeled_images(classes, image_train, label_train, pred_train)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a5b346fa95ed1d00ea589f1cf5c1596b6adfcb39"
      },
      "cell_type": "code",
      "source": "# # for relu\n# def forward_prop(images,parameters):\n#     A=images\n#     L=len(parameters)//2\n#     for i in range(1,L+1):\n#         if(i==L):\n#             print('called',i)#,parameters['W'+str(i)].shape,parameters['b'+str(i)].shape)\n#             A_PREV=A   \n#             print('prev',(np.unique(A_PREV)))\n#             A=linear_activation(A_PREV,parameters['W'+str(i)],parameters['b'+str(i)],'softmax')\n#             print('new',A)\n#         else:\n#             print(i)#parameters['W'+str(i)].shape,parameters['b'+str(i)].shape)\n# #             print(A)\n#             A_PREV=A\n#             print('prev',(np.unique(A_PREV)))\n#             A=linear_activation(A_PREV,parameters['W'+str(i)],parameters['b'+str(i)],'relu')\n#             print('new',np.unique(A))\n#     return A\n# A=forward_prop(image_train,parameters)\n# #print(A)\n    ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b58ab56fdd22dddf2faba519ac6ca84105229f1c"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d1cc80b09d5d7420662187d722d8159030b78d26"
      },
      "cell_type": "code",
      "source": "import torch",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5dacf00ff7806cb9462ffc0cc048816a1f87f3c1"
      },
      "cell_type": "code",
      "source": "import torchvision.models as models\nalexnet = models.alexnet(pretrained=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "070e09481b6836284b05fead55915e2094e556d8"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}